{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "# slim = tf.contrib.slim\n",
    "# FLAGS = tf.app.flags.FLAGS\n",
    "# import urllib2\n",
    "# import sys\n",
    "# import matplotlib.pyplot as plt\n",
    "# import sklearn.metrics as sm\n",
    "\n",
    "# sys.path.append('/home/jt2/Workspace/models/slim')\n",
    "# from datasets import dataset_utils\n",
    "# from nets import inception\n",
    "# from preprocessing import inception_preprocessing\n",
    "# import image_processing\n",
    "from __future__ import division\n",
    "import cervix\n",
    "%aimport utils\n",
    "%aimport train\n",
    "from utils import *\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "set_flag('model', 'inception_v1')\n",
    "# set_flag('cervix_data_dir', '/home/jt2/Workspace/cervix/train/')\n",
    "# set_flag('train_dir', '/tmp/inception_finetuned/'\n",
    "set_flag('image_size', inception.inception_v1.default_image_size)\n",
    "set_flag('data_dir', '/home/jt2/Workspace/cervix/30_img/')\n",
    "set_flag('log_dir', '/tmp/inception_finetuned/30_img')\n",
    "set_flag('checkpoints_dir', '/tmp/checkpoints')\n",
    "set_flag('learning_rate', 0.001)\n",
    "set_flag('number_of_steps', 2)\n",
    "\n",
    "if FLAGS.model == 'inception_v1':\n",
    "    set_flag('url', 'http://download.tensorflow.org/models/inception_v1_2016_08_28.tar.gz')\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From train.py:33: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:394: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:151: add_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n",
      "WARNING:tensorflow:From train.py:34: get_total_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.get_total_loss instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:261: get_losses (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.get_losses instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/losses/python/losses/loss_ops.py:263: get_regularization_losses (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.get_regularization_losses instead.\n",
      "INFO:tensorflow:Summary name losses/Total Loss is illegal; using losses/Total_Loss instead.\n",
      "InceptionV1/Conv2d_1a_7x7/weights:0\n",
      "InceptionV1/Conv2d_1a_7x7/BatchNorm/beta:0\n",
      "InceptionV1/Conv2d_1a_7x7/BatchNorm/moving_mean:0\n",
      "InceptionV1/Conv2d_1a_7x7/BatchNorm/moving_variance:0\n",
      "InceptionV1/Conv2d_2b_1x1/weights:0\n",
      "InceptionV1/Conv2d_2b_1x1/BatchNorm/beta:0\n",
      "InceptionV1/Conv2d_2b_1x1/BatchNorm/moving_mean:0\n",
      "InceptionV1/Conv2d_2b_1x1/BatchNorm/moving_variance:0\n",
      "InceptionV1/Conv2d_2c_3x3/weights:0\n",
      "InceptionV1/Conv2d_2c_3x3/BatchNorm/beta:0\n",
      "InceptionV1/Conv2d_2c_3x3/BatchNorm/moving_mean:0\n",
      "InceptionV1/Conv2d_2c_3x3/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_3b/Branch_0/Conv2d_0a_1x1/weights:0\n",
      "InceptionV1/Mixed_3b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_3b/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_3b/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_3b/Branch_1/Conv2d_0a_1x1/weights:0\n",
      "InceptionV1/Mixed_3b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_3b/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_3b/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_3b/Branch_1/Conv2d_0b_3x3/weights:0\n",
      "InceptionV1/Mixed_3b/Branch_1/Conv2d_0b_3x3/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_3b/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_3b/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_3b/Branch_2/Conv2d_0a_1x1/weights:0\n",
      "InceptionV1/Mixed_3b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_3b/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_3b/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_3b/Branch_2/Conv2d_0b_3x3/weights:0\n",
      "InceptionV1/Mixed_3b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_3b/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_3b/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_3b/Branch_3/Conv2d_0b_1x1/weights:0\n",
      "InceptionV1/Mixed_3b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_3b/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_3b/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_3c/Branch_0/Conv2d_0a_1x1/weights:0\n",
      "InceptionV1/Mixed_3c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_3c/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_3c/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_3c/Branch_1/Conv2d_0a_1x1/weights:0\n",
      "InceptionV1/Mixed_3c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_3c/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_3c/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_3c/Branch_1/Conv2d_0b_3x3/weights:0\n",
      "InceptionV1/Mixed_3c/Branch_1/Conv2d_0b_3x3/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_3c/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_3c/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_3c/Branch_2/Conv2d_0a_1x1/weights:0\n",
      "InceptionV1/Mixed_3c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_3c/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_3c/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_3c/Branch_2/Conv2d_0b_3x3/weights:0\n",
      "InceptionV1/Mixed_3c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_3c/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_3c/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_3c/Branch_3/Conv2d_0b_1x1/weights:0\n",
      "InceptionV1/Mixed_3c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_3c/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_3c/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_4b/Branch_0/Conv2d_0a_1x1/weights:0\n",
      "InceptionV1/Mixed_4b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_4b/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_4b/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_4b/Branch_1/Conv2d_0a_1x1/weights:0\n",
      "InceptionV1/Mixed_4b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_4b/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_4b/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_4b/Branch_1/Conv2d_0b_3x3/weights:0\n",
      "InceptionV1/Mixed_4b/Branch_1/Conv2d_0b_3x3/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_4b/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_4b/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_4b/Branch_2/Conv2d_0a_1x1/weights:0\n",
      "InceptionV1/Mixed_4b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_4b/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_4b/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_4b/Branch_2/Conv2d_0b_3x3/weights:0\n",
      "InceptionV1/Mixed_4b/Branch_2/Conv2d_0b_3x3/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_4b/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_4b/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_4b/Branch_3/Conv2d_0b_1x1/weights:0\n",
      "InceptionV1/Mixed_4b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_4b/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_4b/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_4c/Branch_0/Conv2d_0a_1x1/weights:0\n",
      "InceptionV1/Mixed_4c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_4c/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_4c/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_4c/Branch_1/Conv2d_0a_1x1/weights:0\n",
      "InceptionV1/Mixed_4c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_4c/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_4c/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_4c/Branch_1/Conv2d_0b_3x3/weights:0\n",
      "InceptionV1/Mixed_4c/Branch_1/Conv2d_0b_3x3/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_4c/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_4c/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_4c/Branch_2/Conv2d_0a_1x1/weights:0\n",
      "InceptionV1/Mixed_4c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_4c/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_4c/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_4c/Branch_2/Conv2d_0b_3x3/weights:0\n",
      "InceptionV1/Mixed_4c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_4c/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_4c/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_4c/Branch_3/Conv2d_0b_1x1/weights:0\n",
      "InceptionV1/Mixed_4c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_4c/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_4c/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_4d/Branch_0/Conv2d_0a_1x1/weights:0\n",
      "InceptionV1/Mixed_4d/Branch_0/Conv2d_0a_1x1/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_4d/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_4d/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_4d/Branch_1/Conv2d_0a_1x1/weights:0\n",
      "InceptionV1/Mixed_4d/Branch_1/Conv2d_0a_1x1/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_4d/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_4d/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_4d/Branch_1/Conv2d_0b_3x3/weights:0\n",
      "InceptionV1/Mixed_4d/Branch_1/Conv2d_0b_3x3/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_4d/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_4d/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_4d/Branch_2/Conv2d_0a_1x1/weights:0\n",
      "InceptionV1/Mixed_4d/Branch_2/Conv2d_0a_1x1/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_4d/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_4d/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_4d/Branch_2/Conv2d_0b_3x3/weights:0\n",
      "InceptionV1/Mixed_4d/Branch_2/Conv2d_0b_3x3/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_4d/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_4d/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_4d/Branch_3/Conv2d_0b_1x1/weights:0\n",
      "InceptionV1/Mixed_4d/Branch_3/Conv2d_0b_1x1/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_4d/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_4d/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_4e/Branch_0/Conv2d_0a_1x1/weights:0\n",
      "InceptionV1/Mixed_4e/Branch_0/Conv2d_0a_1x1/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_4e/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_4e/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_4e/Branch_1/Conv2d_0a_1x1/weights:0\n",
      "InceptionV1/Mixed_4e/Branch_1/Conv2d_0a_1x1/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_4e/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_4e/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_4e/Branch_1/Conv2d_0b_3x3/weights:0\n",
      "InceptionV1/Mixed_4e/Branch_1/Conv2d_0b_3x3/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_4e/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_4e/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_4e/Branch_2/Conv2d_0a_1x1/weights:0\n",
      "InceptionV1/Mixed_4e/Branch_2/Conv2d_0a_1x1/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_4e/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_4e/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_4e/Branch_2/Conv2d_0b_3x3/weights:0\n",
      "InceptionV1/Mixed_4e/Branch_2/Conv2d_0b_3x3/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_4e/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_4e/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_4e/Branch_3/Conv2d_0b_1x1/weights:0\n",
      "InceptionV1/Mixed_4e/Branch_3/Conv2d_0b_1x1/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_4e/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_4e/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_4f/Branch_0/Conv2d_0a_1x1/weights:0\n",
      "InceptionV1/Mixed_4f/Branch_0/Conv2d_0a_1x1/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_4f/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_4f/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_4f/Branch_1/Conv2d_0a_1x1/weights:0\n",
      "InceptionV1/Mixed_4f/Branch_1/Conv2d_0a_1x1/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_4f/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_4f/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_4f/Branch_1/Conv2d_0b_3x3/weights:0\n",
      "InceptionV1/Mixed_4f/Branch_1/Conv2d_0b_3x3/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_4f/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_4f/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_4f/Branch_2/Conv2d_0a_1x1/weights:0\n",
      "InceptionV1/Mixed_4f/Branch_2/Conv2d_0a_1x1/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_4f/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_4f/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_4f/Branch_2/Conv2d_0b_3x3/weights:0\n",
      "InceptionV1/Mixed_4f/Branch_2/Conv2d_0b_3x3/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_4f/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_4f/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_4f/Branch_3/Conv2d_0b_1x1/weights:0\n",
      "InceptionV1/Mixed_4f/Branch_3/Conv2d_0b_1x1/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_4f/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_4f/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_5b/Branch_0/Conv2d_0a_1x1/weights:0\n",
      "InceptionV1/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_5b/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_5b/Branch_1/Conv2d_0a_1x1/weights:0\n",
      "InceptionV1/Mixed_5b/Branch_1/Conv2d_0a_1x1/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_5b/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_5b/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_5b/Branch_1/Conv2d_0b_3x3/weights:0\n",
      "InceptionV1/Mixed_5b/Branch_1/Conv2d_0b_3x3/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_5b/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_5b/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_5b/Branch_2/Conv2d_0a_1x1/weights:0\n",
      "InceptionV1/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_5b/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_5b/Branch_2/Conv2d_0a_3x3/weights:0\n",
      "InceptionV1/Mixed_5b/Branch_2/Conv2d_0a_3x3/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_5b/Branch_2/Conv2d_0a_3x3/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_5b/Branch_2/Conv2d_0a_3x3/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_5b/Branch_3/Conv2d_0b_1x1/weights:0\n",
      "InceptionV1/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_5b/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_5c/Branch_0/Conv2d_0a_1x1/weights:0\n",
      "InceptionV1/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_5c/Branch_0/Conv2d_0a_1x1/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_5c/Branch_1/Conv2d_0a_1x1/weights:0\n",
      "InceptionV1/Mixed_5c/Branch_1/Conv2d_0a_1x1/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_5c/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_5c/Branch_1/Conv2d_0a_1x1/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_5c/Branch_1/Conv2d_0b_3x3/weights:0\n",
      "InceptionV1/Mixed_5c/Branch_1/Conv2d_0b_3x3/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_5c/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_5c/Branch_1/Conv2d_0b_3x3/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_5c/Branch_2/Conv2d_0a_1x1/weights:0\n",
      "InceptionV1/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_5c/Branch_2/Conv2d_0a_1x1/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_5c/Branch_2/Conv2d_0b_3x3/weights:0\n",
      "InceptionV1/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_5c/Branch_2/Conv2d_0b_3x3/BatchNorm/moving_variance:0\n",
      "InceptionV1/Mixed_5c/Branch_3/Conv2d_0b_1x1/weights:0\n",
      "InceptionV1/Mixed_5c/Branch_3/Conv2d_0b_1x1/BatchNorm/beta:0\n",
      "InceptionV1/Mixed_5c/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_mean:0\n",
      "InceptionV1/Mixed_5c/Branch_3/Conv2d_0b_1x1/BatchNorm/moving_variance:0\n",
      "INFO:tensorflow:Starting Session.\n",
      "INFO:tensorflow:Starting Queues.\n",
      "INFO:tensorflow:global_step/sec: 0\n",
      "INFO:tensorflow:global step 1: loss = 1.4314 (23.45 sec/step)\n",
      "INFO:tensorflow:global step 2: loss = 1.0381 (9.74 sec/step)\n",
      "INFO:tensorflow:Stopping Training.\n",
      "INFO:tensorflow:Finished training! Saving model to disk.\n",
      "Finished training. Last batch loss 1.038126\n"
     ]
    }
   ],
   "source": [
    "Train(cervix).init_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# init train\n",
    "with tf.Graph().as_default():\n",
    "    dataset = cervix.get_split('train', cervix_data_dir)\n",
    "    images, _, labels = load_batch(dataset, height=image_size, width=image_size)\n",
    "    \n",
    "#     Create the model, use the default arg scope to configure the batch norm parameters.\n",
    "    with slim.arg_scope(inception.inception_v1_arg_scope()):\n",
    "        logits, _ = inception.inception_v1(images, num_classes=dataset.num_classes, is_training=True)\n",
    "        \n",
    "    labels = tf.subtract(labels, 1)\n",
    "    \n",
    "    # Specify the loss function:\n",
    "    one_hot_labels = slim.one_hot_encoding(labels, dataset.num_classes)\n",
    "    one_hot_labels = tf.cast(one_hot_labels, tf.int64)\n",
    "    slim.losses.softmax_cross_entropy(logits, one_hot_labels)\n",
    "    total_loss = slim.losses.get_total_loss()\n",
    "\n",
    "    # Create some summaries to visualize the training process:\n",
    "    tf.summary.scalar('losses/Total Loss', total_loss)\n",
    "  \n",
    "    # Specify the optimizer and create the train op:\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "    train_op = slim.learning.create_train_op(total_loss, optimizer)\n",
    "    \n",
    "    # Run the training:\n",
    "    final_loss = slim.learning.train(\n",
    "        train_op,\n",
    "        logdir=log_dir,\n",
    "        init_fn=get_init_fn(),\n",
    "        number_of_steps=FLAGS.number_of_steps)\n",
    "        \n",
    "  \n",
    "print('Finished training. Last batch loss %f' % final_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# train from checkpoint\n",
    "with tf.Graph().as_default():\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    \n",
    "    dataset = cervix.get_split('train', cervix_data_dir)\n",
    "    images, _, labels, _,_ = load_batch(dataset, height=image_size, width=image_size)\n",
    "    \n",
    "#     Create the model, use the default arg scope to configure the batch norm parameters.\n",
    "    with slim.arg_scope(inception.inception_v1_arg_scope()):\n",
    "        logits, _ = inception.inception_v1(images, num_classes=dataset.num_classes, is_training=True)\n",
    "        \n",
    "    labels = tf.subtract(labels, 1)\n",
    "    \n",
    "    # Specify the loss function:\n",
    "    one_hot_labels = slim.one_hot_encoding(labels, dataset.num_classes)\n",
    "    slim.losses.softmax_cross_entropy(logits, one_hot_labels)\n",
    "    total_loss = slim.losses.get_total_loss()\n",
    "\n",
    "    # Create some summaries to visualize the training process:\n",
    "    tf.summary.scalar('losses/Total Loss', total_loss)\n",
    "  \n",
    "    # Specify the optimizer and create the train op:\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "    train_op = slim.learning.create_train_op(total_loss, optimizer)\n",
    "    \n",
    "    # Run the training:\n",
    "    final_loss = slim.learning.train(\n",
    "        train_op,\n",
    "        logdir=train_dir,\n",
    "        number_of_steps=200)\n",
    "        \n",
    "  \n",
    "print('Finished training. Last batch loss %f' % final_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#EVALUATE TRAINED MODEL\n",
    "image_size = inception.inception_v1.default_image_size\n",
    "batch_size = 1\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    \n",
    "#     dataset = cervix.get_split('train', cervix_data_dir)\n",
    "    dataset = cervix.get_split('train', '/home/jt2/Workspace/cervix/30_img/')\n",
    "#     images, images_raw, labels = load_batch(dataset, height=image_size, width=image_size, is_training=False)\n",
    "    images, images_raw, labels, labels_name, filenames = load_batch(dataset, \n",
    "                                                           batch_size=30, \n",
    "                                                           shuffle=False, \n",
    "                                                           height=image_size, \n",
    "                                                           width=image_size, \n",
    "                                                           is_training=True)\n",
    "    \n",
    "    labels = tf.subtract(labels, 1)\n",
    "    \n",
    "    one_hot_labels = slim.one_hot_encoding(labels, dataset.num_classes)\n",
    "    \n",
    "    # Create the model, use the default arg scope to configure the batch norm parameters.\n",
    "    with slim.arg_scope(inception.inception_v1_arg_scope()):\n",
    "        logits, _ = inception.inception_v1(images, num_classes=dataset.num_classes, is_training=False)\n",
    "\n",
    "    probabilities = tf.nn.softmax(logits)\n",
    "    \n",
    "    checkpoint_path = tf.train.latest_checkpoint(train_dir)\n",
    "    init_fn = slim.assign_from_checkpoint_fn(\n",
    "      checkpoint_path,\n",
    "      slim.get_variables_to_restore())\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        with slim.queues.QueueRunners(sess):\n",
    "            sess.run(tf.initialize_local_variables())\n",
    "            init_fn(sess)\n",
    "#             np_probabilities, np_images_raw, np_labels = sess.run([probabilities, images_raw, labels])\n",
    "            np_probabilities, np_images_raw, np_labels, np_images, logit_train, one_hot, label_name, filename = sess.run([probabilities, images_raw, labels, images, logits, one_hot_labels, labels_name, filenames])\n",
    "    \n",
    "            for i in xrange(batch_size): \n",
    "                image = np_images_raw[i, :, :, :]\n",
    "                image_train = np_images[i, :, :, :]\n",
    "                true_label = np_labels[i]\n",
    "                predicted_label = np.argmax(np_probabilities[i, :])\n",
    "                print one_hot\n",
    "                print logit_train\n",
    "                print np_probabilities\n",
    "                print np.argmax(np_probabilities, axis=1)\n",
    "                print np_labels\n",
    "                print \"label_name\"\n",
    "                print label_name\n",
    "                print \"filename\"\n",
    "                print filename\n",
    "                \n",
    "#                 predicted_name = dataset.labels_to_names[predicted_label]\n",
    "#                 true_name = dataset.labels_to_names[true_label]\n",
    "                \n",
    "                plt.figure()\n",
    "                plt.imshow(image.astype(np.uint8))\n",
    "#                 plt.title('Ground Truth: [%s], Prediction [%s]' % (true_name, predicted_name))\n",
    "                plt.title('Ground Truth: [%s], Prediction [%s]' % (true_label, predicted_label))\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "            \n",
    "            print(sm.classification_report(np_labels, np.argmax(np_probabilities, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "pred = np.argmax(np_probabilities,axis=1)\n",
    "is_right = np.equal(np_labels, pred)\n",
    "log_loss = sm.log_loss(np_labels, np_probabilities)\n",
    "print \"Accuracy: %f\" % (np.count_nonzero(is_right)/is_right.shape[0])\n",
    "print \"Log loss: %f\" % log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "index_1 = np.where(np_labels == 0)\n",
    "index_2 = np.where(np_labels == 1)\n",
    "index_3 = np.where(np_labels == 2)\n",
    "df1 = pd.DataFrame(np_probabilities[index_1], index=filename[index_1])\n",
    "df1 = df1.sort_index()\n",
    "df2 = pd.DataFrame(np_probabilities[index_2], index=filename[index_2])\n",
    "df2 = df2.sort_index()\n",
    "df3 = pd.DataFrame(np_probabilities[index_3], index=filename[index_3])\n",
    "df3 = df3.sort_index()\n",
    "print df1\n",
    "print df2\n",
    "print df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print image_train.shape\n",
    "image\n",
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3,sharey=True)\n",
    "ax1.imshow(image_train[:,:,0], cmap='gray')\n",
    "ax2.imshow(image_train[:,:,1], cmap='gray')\n",
    "ax3.imshow(image_train[:,:,2], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#CHECK WEIGHTS\n",
    "with tf.Graph().as_default():\n",
    "    tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    \n",
    "#     dataset = cervix.get_split('train', cervix_data_dir)\n",
    "    images, images_raw, labels = load_batch(dataset, shuffle=True, height=image_size, width=image_size, is_training=True)\n",
    "    \n",
    "    # Create the model, use the default arg scope to configure the batch norm parameters.\n",
    "    with slim.arg_scope(inception.inception_v1_arg_scope()):\n",
    "        logits, _ = inception.inception_v1(images, num_classes=dataset.num_classes, is_training=False)\n",
    "\n",
    "    checkpoint_path = tf.train.latest_checkpoint(train_dir)\n",
    "    init_fn = slim.assign_from_checkpoint_fn(\n",
    "      checkpoint_path,\n",
    "      slim.get_variables_to_restore())\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        with slim.queues.QueueRunners(sess):\n",
    "            sess.run(tf.initialize_local_variables())\n",
    "            init_fn(sess)\n",
    "#             in_logits = slim.get_variables_by_name('InceptionV1/Logits/Conv2d_0c_1x1/weights')\n",
    "#             stay_same = slim.get_variables_by_name('InceptionV1/Mixed_5c/Branch_3/Conv2d_0b_1x1/weights')\n",
    "            variables = slim.get_variables()\n",
    "#             l_out, ss = sess.run([in_logits, stay_same])\n",
    "            all_var = sess.run(variables)\n",
    "        \n",
    "        print \"DONE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "get_variables_stats(variables, all_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "utils??\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
